import requests
from bs4 import BeautifulSoup
import openai
import os
import json
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse

openai.api_key = os.getenv("OPENAI_API_KEY")

SOURCE_URL = "https://theskint.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}
OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def fetch_all_articles():
    page = 1
    all_articles = []

    while True:
        url = f"{SOURCE_URL}/page/{page}/" if page > 1 else SOURCE_URL
        print(f"ğŸŒ Fetching: {url}")
        try:
            resp = requests.get(url, headers=HEADERS, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")
            articles = soup.find_all("article")

            if not articles:
                print(f"ğŸš« No <article> found on page {page}, stopping.")
                break

            print(f"âœ… Page {page}: Found {len(articles)} <article> blocks")
            all_articles.extend(articles)
            page += 1
        except Exception as e:
            print(f"âŒ Failed to fetch page {page}: {e}")
            break

    return all_articles

def extract_all_links_from_article(article):
    """æå–æ–‡ç« ä¸­æ‰€æœ‰çš„é“¾æ¥ï¼Œç‰¹åˆ«å…³æ³¨ >> é“¾æ¥"""
    links = []
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„é“¾æ¥
    all_links = article.find_all("a", href=True)
    
    for a in all_links:
        href = a.get("href", "").strip()
        link_text = a.get_text(strip=True)
        
        # è·³è¿‡ç©ºé“¾æ¥
        if not href:
            continue
            
        # å¤„ç†ç›¸å¯¹é“¾æ¥
        if href.startswith('/'):
            href = urljoin(SOURCE_URL, href)
        elif not href.startswith(('http://', 'https://')):
            href = urljoin(SOURCE_URL, href)
        
        # è®°å½•é“¾æ¥ä¿¡æ¯
        links.append({
            'url': href,
            'text': link_text,
            'is_read_more': link_text == ">>"
        })
    
    return links

def extract_article_link(article):
    """æå–æ–‡ç« çš„ä¸»è¦é“¾æ¥ï¼Œä¼˜å…ˆé€‰æ‹© >> é“¾æ¥"""
    links = extract_all_links_from_article(article)
    
    # ä¼˜å…ˆæŸ¥æ‰¾ >> é“¾æ¥
    for link in links:
        if link['is_read_more']:
            return link['url']
    
    # å¦‚æœæ²¡æœ‰ >> é“¾æ¥ï¼Œå°è¯•ä»æ ‡é¢˜ä¸­è·å–é“¾æ¥
    h2 = article.find("h2")
    if h2:
        a = h2.find("a", href=True)
        if a and a["href"]:
            href = a["href"].strip()
            if href.startswith('/'):
                href = urljoin(SOURCE_URL, href)
            elif not href.startswith(('http://', 'https://')):
                href = urljoin(SOURCE_URL, href)
            return href
    
    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„å¤–éƒ¨é“¾æ¥
    for link in links:
        if link['url'] != SOURCE_URL and not link['url'].endswith('#'):
            return link['url']
    
    return SOURCE_URL

def extract_text_from_articles(articles):
    event_texts = []
    for i, article in enumerate(articles):
        title_el = article.find("h2") or article.find("h1")
        title = title_el.get_text(strip=True) if title_el else "Untitled"

        # è·å–ä¸»è¦é“¾æ¥
        main_link = extract_article_link(article)
        
        # è·å–æ‰€æœ‰é“¾æ¥ä¿¡æ¯
        all_links = extract_all_links_from_article(article)

        content_el = article.find("div", class_="post-content") or article
        content = content_el.get_text(separator="\n", strip=True)

        # æ„å»ºå®Œæ•´æ–‡æœ¬ï¼ŒåŒ…å«æ‰€æœ‰é“¾æ¥ä¿¡æ¯
        links_info = "\n".join([f"Link: {link['text']} -> {link['url']}" for link in all_links])
        
        full_text = f"{title}\n\n{content}\n\nMain link: {main_link}\n\nAll links:\n{links_info}"

        if len(content) > 80:
            event_texts.append(full_text)
        else:
            print(f"âš ï¸ Skipped article {i+1}, content too short")

    print(f"ğŸ“ Extracted {len(event_texts)} usable article blocks")
    return event_texts

def extract_event_summary(text):
    """æ”¹è¿›çš„GPTæç¤ºï¼Œæ›´å¥½åœ°å¤„ç†é“¾æ¥"""
    prompt = f"""
From the following article text, extract and summarize only the **free public events in New York City**.
Pay special attention to the links provided in the text - use the actual URLs from the "All links" section.

Respond only in markdown bullet list format like this:

- ğŸ‰ **Event Title**  
  ğŸ“ Location  
  ğŸ•’ Time / Date  
  ğŸ“ Description  
  ğŸ”— [Link](actual_url_from_the_text)

Important: Use the ACTUAL URLs from the "All links" section, not placeholder links.
If no free NYC events are found, return nothing.

Text:
{text[:4000]}
"""

    try:
        result = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        content = result.choices[0].message.content.strip()

        # æ¸…ç†markdownä»£ç å—
        if content.startswith("```markdown"):
            content = content.removeprefix("```markdown").strip()
        if content.startswith("```"):
            content = content.removeprefix("```").strip()
        if content.endswith("```"):
            content = content.removesuffix("```").strip()

        print(f"ğŸ§¾ GPT result sample:\n{content[:150]}...\n")
        return content
    except Exception as e:
        print(f"âŒ GPT call failed: {e}")
        return ""

def save_outputs(markdown_data, all_articles, today):
    """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
    with open(f"{OUTPUT_DIR}/events_gpt_{today}.md", "w", encoding="utf-8") as mf:
        mf.write(markdown_data)
    with open(f"{OUTPUT_DIR}/events_gpt_{today}.json", "w", encoding="utf-8") as jf:
        json.dump(all_articles, jf, indent=2, ensure_ascii=False)
    print("âœ… Output files saved to /output")

def main():
    print("ğŸš€ Starting The Skint crawler...")
    
    # è·å–æ–‡ç« 
    articles = fetch_all_articles()
    print(f"ğŸ“° Total articles found: {len(articles)}")
    
    # æå–æ–‡æœ¬
    article_texts = extract_text_from_articles(articles)

    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    summaries = []
    for i, text in enumerate(article_texts):
        print(f"ğŸ” Processing article {i+1}/{len(article_texts)}")
        summary = extract_event_summary(text)
        if summary:
            summaries.append(summary)

    # ä¿å­˜ç»“æœ
    today = datetime.now().strftime("%Y-%m-%d")
    final_md = "\n\n".join(summaries)
    save_outputs(final_md, article_texts, today)

    print(f"ğŸ¯ Script completed. Generated {len(summaries)} event summaries.")

if __name__ == "__main__":
    main()
