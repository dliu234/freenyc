import requests
from bs4 import BeautifulSoup
import openai
import os
import json
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse, unquote
import html

openai.api_key = os.getenv("OPENAI_API_KEY")

SOURCE_URL = "https://theskint.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}
OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def fetch_all_articles():
    page = 1
    all_articles = []

    while True:
        url = f"{SOURCE_URL}/page/{page}/" if page > 1 else SOURCE_URL
        print(f"ğŸŒ Fetching: {url}")
        try:
            resp = requests.get(url, headers=HEADERS, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")
            articles = soup.find_all("article")

            if not articles:
                print(f"ğŸš« No <article> found on page {page}, stopping.")
                break

            print(f"âœ… Page {page}: Found {len(articles)} <article> blocks")
            all_articles.extend(articles)
            page += 1
        except Exception as e:
            print(f"âŒ Failed to fetch page {page}: {e}")
            break

    return all_articles

def clean_and_validate_url(url, base_url=SOURCE_URL):
    """æ¸…ç†å’ŒéªŒè¯URL"""
    if not url or not url.strip():
        return None
    
    url = url.strip()
    
    # è·³è¿‡æ— æ•ˆçš„é“¾æ¥ç±»å‹
    if (url.startswith('javascript:') or 
        url == '#' or 
        url.startswith('mailto:') or
        url.startswith('tel:')):
        return None
    
    # è§£ç HTMLå®ä½“
    url = html.unescape(url)
    
    # ç§»é™¤å¯èƒ½çš„æˆªæ–­æ ‡è¯†
    if url.endswith('...'):
        return None
    
    # å¤„ç†ç›¸å¯¹é“¾æ¥
    if url.startswith('/'):
        url = urljoin(base_url, url)
    elif not url.startswith(('http://', 'https://')):
        url = urljoin(base_url, url)
    
    # éªŒè¯URLæ ¼å¼
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
    except:
        return None
    
    return url

def extract_all_links_with_context(article):
    """æå–æ–‡ç« ä¸­æ‰€æœ‰é“¾æ¥ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    content_el = article.find("div", class_="post-content") or article
    content_text = content_el.get_text(separator="\n", strip=True)
    
    # è·å–æ‰€æœ‰ HTML é“¾æ¥
    all_links = article.find_all("a", href=True)
    links_with_context = []
    
    for a in all_links:
        href = a.get("href", "").strip()
        link_text = a.get_text(strip=True)
        
        # æ¸…ç†å’ŒéªŒè¯URL
        clean_url = clean_and_validate_url(href)
        
        if clean_url and link_text:  # ç¡®ä¿é“¾æ¥æ–‡æœ¬ä¸ä¸ºç©º
            # å°è¯•æ‰¾åˆ°é“¾æ¥å‰çš„æè¿°æ–‡æœ¬
            context = ""
            if a.parent:
                # è·å–é“¾æ¥æ‰€åœ¨æ®µè½çš„æ–‡æœ¬
                paragraph_text = a.parent.get_text(strip=True)
                
                # å°è¯•æå–é“¾æ¥å‰çš„äº‹ä»¶æè¿°
                # å¯»æ‰¾æ¨¡å¼å¦‚ "event name: description. [>>]"
                before_link = ""
                if link_text and link_text in paragraph_text:
                    before_link = paragraph_text.split(link_text)[0]
                
                # æ¸…ç†ä¸Šä¸‹æ–‡ï¼Œå–æœ€åçš„äº‹ä»¶æè¿°
                lines = before_link.split('\n')
                for line in reversed(lines):
                    line = line.strip()
                    if line and len(line) > 10:  # è‡³å°‘10ä¸ªå­—ç¬¦çš„æè¿°
                        # å¯»æ‰¾äº‹ä»¶æ¨¡å¼ï¼Œé€šå¸¸åŒ…å«æ—¶é—´å’Œåœ°ç‚¹ä¿¡æ¯
                        if any(indicator in line.lower() for indicator in ['pm:', 'am:', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']):
                            context = line[:100]  # å–å‰100ä¸ªå­—ç¬¦
                            break
            
            links_with_context.append({
                'url': clean_url,
                'text': link_text,
                'context': context,
                'is_read_more': link_text == ">>",
                'raw_href': href
            })
    
    return links_with_context

def create_event_link_mapping(article):
    """åˆ›å»ºäº‹ä»¶åˆ°é“¾æ¥çš„æ˜ å°„"""
    content_el = article.find("div", class_="post-content") or article
    content_html = str(content_el)
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« >> é“¾æ¥çš„æ®µè½
    events_with_links = []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°æ‰€æœ‰ >> é“¾æ¥åŠå…¶ä¸Šä¸‹æ–‡
    link_pattern = r'<a[^>]*href="([^"]*)"[^>]*>>.*?</a>'
    matches = re.finditer(link_pattern, content_html, re.IGNORECASE | re.DOTALL)
    
    for match in matches:
        url = clean_and_validate_url(match.group(1))
        if url:
            # è·å–é“¾æ¥å‰çš„æ–‡æœ¬
            before_link = content_html[:match.start()]
            
            # æŸ¥æ‰¾æœ€è¿‘çš„äº‹ä»¶æè¿°
            # å¯»æ‰¾åŒ…å«æ—¶é—´ä¿¡æ¯çš„è¡Œ
            lines = before_link.split('\n')
            event_description = ""
            
            for line in reversed(lines):
                line_text = BeautifulSoup(line, 'html.parser').get_text(strip=True)
                if line_text and any(time_indicator in line_text.lower() for time_indicator in 
                                   ['pm:', 'am:', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']):
                    event_description = line_text[:150]  # å‰150ä¸ªå­—ç¬¦
                    break
            
            if event_description:
                events_with_links.append({
                    'description': event_description,
                    'url': url
                })
    
    return events_with_links

def extract_text_from_articles(articles):
    event_texts = []
    for i, article in enumerate(articles):
        title_el = article.find("h2") or article.find("h1")
        title = title_el.get_text(strip=True) if title_el else "Untitled"

        content_el = article.find("div", class_="post-content") or article
        content = content_el.get_text(separator="\n", strip=True)

        # è·å–äº‹ä»¶-é“¾æ¥æ˜ å°„
        event_links = create_event_link_mapping(article)
        
        # è·å–æ‰€æœ‰é“¾æ¥ï¼ˆç”¨ä½œå¤‡é€‰ï¼‰
        all_links = extract_all_links_with_context(article)
        
        # æ„å»ºè¯¦ç»†çš„é“¾æ¥æ˜ å°„ä¿¡æ¯
        links_mapping = []
        if event_links:
            for j, event_link in enumerate(event_links):
                links_mapping.append(f"Event {j+1}: {event_link['description']} -> {event_link['url']}")
        
        all_links_info = []
        for j, link in enumerate(all_links):
            if link['is_read_more']:
                all_links_info.append(f">> Link {j+1}: {link['context']} -> {link['url']}")
        
        full_text = f"""Title: {title}

Content:
{content}

Event-Link Mappings:
{chr(10).join(links_mapping) if links_mapping else "No specific event-link mappings found"}

All >> Links:
{chr(10).join(all_links_info) if all_links_info else "No >> links found"}

Default Link: {all_links[0]['url'] if all_links else SOURCE_URL}
"""

        if len(content) > 80:
            event_texts.append(full_text)
            print(f"âœ… Article {i+1}: {title[:50]}... ({len(event_links)} event-link mappings)")
        else:
            print(f"âš ï¸ Skipped article {i+1}, content too short")

    print(f"ğŸ“ Extracted {len(event_texts)} usable article blocks")
    return event_texts

def extract_event_summary(text):
    """æ”¹è¿›çš„GPTæç¤ºï¼Œä½¿ç”¨äº‹ä»¶-é“¾æ¥æ˜ å°„"""
    prompt = f"""
From the following article text, extract and summarize only the **free public events in New York City**.

IMPORTANT LINKING INSTRUCTIONS:
1. Use the "Event-Link Mappings" section to match each event with its specific URL
2. If an event doesn't have a specific mapping, use one of the "All >> Links" that seems most relevant
3. If neither exists, use the "Default Link"
4. DO NOT use the same URL for multiple different events unless they are truly the same event

Respond only in markdown bullet list format like this:

- ğŸ‰ **Event Title**  
  ğŸ“ Location  
  ğŸ•’ Time / Date  
  ğŸ“ Description  
  ğŸ”— [Link](use_specific_url_for_this_event)

Rules:
- Only include FREE events in NYC
- Each event should have its own specific URL when possible
- Match events to their corresponding URLs from the mappings
- If no free NYC events are found, return nothing

Text:
{text}
"""

    try:
        result = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1  # é™ä½æ¸©åº¦ä»¥æé«˜ä¸€è‡´æ€§
        )
        content = result.choices[0].message.content.strip()

        # æ¸…ç†markdownä»£ç å—
        content = re.sub(r'^```markdown\s*', '', content)
        content = re.sub(r'^```\s*', '', content)
        content = re.sub(r'\s*```$', '', content)

        return content
    except Exception as e:
        print(f"âŒ GPT call failed: {e}")
        return ""

def save_outputs(markdown_data, all_articles, today):
    """ä¿å­˜è¾“å‡º"""
    with open(f"{OUTPUT_DIR}/events_fixed_{today}.md", "w", encoding="utf-8") as mf:
        mf.write("# NYC Free Events - The Skint (Fixed Links)\n\n")
        mf.write(f"Generated on: {today}\n\n")
        mf.write(markdown_data if markdown_data.strip() else "No events found.")
    
    with open(f"{OUTPUT_DIR}/events_raw_{today}.json", "w", encoding="utf-8") as jf:
        json.dump(all_articles, jf, indent=2, ensure_ascii=False)
    
    print("âœ… Fixed version files saved")

def main():
    print("ğŸš€ Starting The Skint crawler (Fixed Link Version)...")
    
    articles = fetch_all_articles()
    print(f"ğŸ“° Total articles found: {len(articles)}")
    
    article_texts = extract_text_from_articles(articles)

    summaries = []
    for i, text in enumerate(article_texts):
        print(f"ğŸ” Processing article {i+1}/{len(article_texts)}")
        summary = extract_event_summary(text)
        if summary and summary.strip():
            summaries.append(summary)
            print(f"âœ… Generated summary")
        else:
            print(f"âŒ No summary generated")

    today = datetime.now().strftime("%Y-%m-%d")
    final_md = "\n\n---\n\n".join(summaries) if summaries else "No events found."
    save_outputs(final_md, article_texts, today)

    print(f"\nğŸ¯ Fixed version completed!")
    print(f"   ğŸ“Š Generated {len(summaries)} event summaries")
    print(f"   ğŸ“ Files saved in '{OUTPUT_DIR}' directory")
    print(f"   ğŸ”— Each event should now have its specific link")

if __name__ == "__main__":
    main()
